{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "authorship_tag": "ABX9TyOjphShzelfBBQEg/PulGou",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "premium",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tobb-e/machine-learning/blob/main/Training_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title # Connect Google Drive\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "91Za3CwUYDQO",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%env WANDB_LOG_MODEL=true"
      ],
      "metadata": {
        "id": "vp8l9Gu3a1sZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install huggingface_hub==0.11.0 -q\n",
        "%pip install transformers -q\n",
        "%pip install scikit-learn -q\n",
        "%pip install wandb -q\n",
        "import wandb\n",
        "%load_ext wandb\n",
        "wandb.login()"
      ],
      "metadata": {
        "id": "Cnc_XGncIb9y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "%%wandb\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from transformers import DataCollatorForLanguageModeling, Trainer, TrainingArguments, TrainerCallback\n",
        "from torch.optim import AdamW  # Import AdamW from torch.optim\n",
        "from tqdm import tqdm\n",
        "\n",
        "class WandbCallback(TrainerCallback):\n",
        "    def on_log(self, args, state, control, model=None, logs=None, **kwargs):\n",
        "        logs = logs or {}\n",
        "        wandb.log(logs, step=state.global_step)\n",
        "\n",
        "class CustomDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings):\n",
        "        self.encodings = encodings\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {key: val[idx].clone().detach() for key, val in self.encodings.items()}\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encodings.input_ids)\n",
        "\n",
        "run = wandb.init(\n",
        "      project=\"python_model\",\n",
        "      config={\n",
        "      \"batch_size\": 128,\n",
        "      \"learning_rate\": 0.01,\n",
        "      \"dataset\": \"python_code\",\n",
        "})\n",
        "\n",
        "os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"./train_model_wandb.ipynb\"\n",
        "\n",
        "# Load the GPT-2 tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "# Load the GPT-2 model\n",
        "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
        "\n",
        "# Add special tokens and update the tokenizer's vocabulary\n",
        "special_tokens_dict = {'pad_token': '[PAD]'}\n",
        "num_added_tokens = tokenizer.add_special_tokens(special_tokens_dict)\n",
        "\n",
        "# Resize token embeddings\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "# Save the tokenizer to update the vocabulary size\n",
        "tokenizer.save_pretrained('gdrive/MyDrive/machine_learning/models')\n",
        "\n",
        "# save your trained model checkpoint to wandb\n",
        "os.environ[\"WANDB_LOG_MODEL\"]=\"true\"\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('gdrive/MyDrive/machine_learning/csv/python_code02.csv')\n",
        "\n",
        "# Split the dataset into training and validation sets\n",
        "train_data, val_data = train_test_split(data, test_size=0.25, random_state=42)\n",
        "\n",
        "# Tokenize the training and validation datasets with a max_length\n",
        "max_length = 512\n",
        "train_encodings = tokenizer(train_data['Snippet'].tolist(), truncation=True, padding=True, max_length=max_length, return_tensors='pt')\n",
        "val_encodings = tokenizer(val_data['Snippet'].tolist(), truncation=True, padding=True, max_length=max_length, return_tensors='pt')\n",
        "\n",
        "# Create the custom dataset objects\n",
        "train_dataset = CustomDataset(train_encodings)\n",
        "val_dataset = CustomDataset(val_encodings)\n",
        "\n",
        "# Create the data collator\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "\n",
        "# Set up the training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='gdrive/MyDrive/machine_learning/models',\n",
        "    evaluation_strategy=\"steps\",\n",
        "    save_strategy=\"steps\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=32,\n",
        "#    gradient_accumulation_steps=4,\n",
        "    gradient_checkpointing=True,\n",
        "    num_train_epochs=3,  # Number of complete passes through the training dataset\n",
        "    weight_decay=0.01,\n",
        "    push_to_hub=False,\n",
        "    logging_dir='gdrive/MyDrive/machine_learning/logs',\n",
        "    report_to=\"wandb\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    greater_is_better=False,\n",
        "    save_total_limit=2,\n",
        "    save_steps=1000,\n",
        "    eval_steps=1000,\n",
        "    fp16=True, # Enable mixed precision training\n",
        ")\n",
        "\n",
        "# Create the Trainer object\n",
        "trainer = Trainer(\n",
        "    callbacks=[WandbCallback()],  # Add the WandbCallback here\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    data_collator=data_collator,\n",
        "    eval_dataset=val_dataset,\n",
        "    optimizers=(AdamW(model.parameters(), lr=2e-5), None),  # Pass the AdamW optimizer\n",
        ")\n",
        "\n",
        "# Fine-tune the model on the training set\n",
        "progress_bar = tqdm(range(training_args.num_train_epochs), desc=\"Training\")\n",
        "for epoch in progress_bar:\n",
        "    trainer.train()\n",
        "    progress_bar.set_description(\"Epoch {}\".format(epoch+1))\n",
        "\n",
        "wandb.log({\"steps\": steps, \"loss\": loss, \"eval\": eval})\n",
        "\n",
        "# [optional] finish the wandb run, necessary in notebooks\n",
        "wandb.finish()"
      ],
      "metadata": {
        "id": "8SVxvcz_VIP4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}